### *1. What happens when the number of hidden nodes increase?*
**Increased Capacity:**
 In general, increasing the number of hidden nodes increases the model's capacity to learn and represent more complex patterns in the data. Each hidden node can be viewed as learning a different feature or aspect of the data. This means that with more nodes, the network can probably capture relationships that are more detailed and nuanced.

**Better Fit Up to a Certain Point:** Since this network has more hidden nodes, it may initially fit the training data better as the training accuracy goes up, because it has more parameters-weights that can be adjusted in order to minimize the training error.

**Overfitting Risk:** 
If the number of hidden nodes continues to increase, the model may get overly complex. It could even start overfitting the training data by learning underlying patterns and noise, usually leading to a decrease in the validation or test accuracy since it does not generalize well on unseen data.

**Increased Training Time:** The more hidden nodes there are, the more parameters there are, and hence, computational power and computational time to train the model increase. Gradient descent would now have more parameters to update and may converge even more slowly.


### *2. Can you explain the pattern of the accuracy when the hidden nodes increase?*

This is because, in general, the more hidden nodes a neural network possesses, the more the accuracy will increase initially before it levels off and probably decreases afterwards. When the number of hidden nodes goes up initially, a network can capture more complex patterns, reflecting in an increase for both training and validation accuracy. However, after a certain optimum number of nodes is reached, the accuracy levels because the model has enough capacity to learn the underlying patterns in the data. Further increases beyond this number may cause the model to start overfitting the training data; as such, while the validation accuracy drops, training accuracy might still increase. This shows a familiar pattern: with an increase in model complexity, there is an optimal balance point at which performance increases in generalization: too few nodes are subject to underfitting, while too many give rise to overfitting.